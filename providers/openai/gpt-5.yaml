# OpenAI GPT-5 Provider Specification
# Copyright (c) 2025 Specado Team
# Licensed under the Apache-2.0 license

spec_version: "1.1.0"

provider:
  name: openai
  base_url: "https://api.openai.com"
  headers:
    Content-Type: "application/json"
    OpenAI-Beta: "assistants=v2"
  auth:
    type: bearer
    header_name: Authorization
    value_template: "Bearer ${ENV:OPENAI_API_KEY}"
  capabilities:
    supports_rag: true
    supports_conversation_persistence: false
    supports_live_sessions: false
    supports_multimodal_output: false

models:
  - id: gpt-5
    aliases:
      - gpt-5-turbo
      - gpt-5-latest
    family: gpt-5
    
    endpoints:
      chat_completion:
        method: POST
        path: "/v1/chat/completions"
        protocol: https
      streaming_chat_completion:
        method: POST
        path: "/v1/chat/completions"
        protocol: https
        headers:
          Accept: "text/event-stream"
    
    input_modes:
      messages: true
      single_text: false
      images: true
      audio: false
      video: false
      documents: false
    
    tooling:
      tools_supported: true
      parallel_tool_calls_default: true
      can_disable_parallel_tool_calls: true
      tool_choice_modes:
        - auto
        - none
        - required
        - specific
      strict_tools_support: true
      disable_switch:
        path: "parallel_tool_calls"
        value: false
    
    json_output:
      native_param: true
      strategy: json_schema
      notes: "Supports JSON Schema response format with strict validation"
    
    parameters:
      # Core generation parameters
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 1
      top_p:
        type: number
        minimum: 0
        maximum: 1
        default: 1
      frequency_penalty:
        type: number
        minimum: -2
        maximum: 2
        default: 0
      presence_penalty:
        type: number
        minimum: -2
        maximum: 2
        default: 0
      max_tokens:
        type: integer
        minimum: 1
        maximum: 128000
        default: null
      n:
        type: integer
        minimum: 1
        maximum: 10
        default: 1
      stop:
        type: array
        items:
          type: string
        maxItems: 4
      seed:
        type: integer
        minimum: 0
        maximum: 2147483647
      
      # Response format
      response_format:
        type: object
        properties:
          type:
            type: string
            enum: ["text", "json_object", "json_schema"]
          json_schema:
            type: object
      
      # Tool configuration
      tools:
        type: array
        items:
          type: object
          required: ["type", "function"]
          properties:
            type:
              type: string
              enum: ["function"]
            function:
              type: object
              required: ["name", "description", "parameters"]
              properties:
                name:
                  type: string
                description:
                  type: string
                parameters:
                  type: object
                strict:
                  type: boolean
                  default: false
      
      tool_choice:
        oneOf:
          - type: string
            enum: ["none", "auto", "required"]
          - type: object
            properties:
              type:
                type: string
                enum: ["function"]
              function:
                type: object
                properties:
                  name:
                    type: string
      
      parallel_tool_calls:
        type: boolean
        default: true
      
      # Streaming
      stream:
        type: boolean
        default: false
      
      stream_options:
        type: object
        properties:
          include_usage:
            type: boolean
            default: false
      
      # Safety
      modalities:
        type: array
        items:
          type: string
          enum: ["text", "image"]
        default: ["text"]
      
      # Logging
      logit_bias:
        type: object
        additionalProperties:
          type: number
          minimum: -100
          maximum: 100
      
      logprobs:
        type: boolean
        default: false
      
      top_logprobs:
        type: integer
        minimum: 0
        maximum: 20
      
      # User tracking
      user:
        type: string
        description: "Unique identifier for end-user tracking"
    
    constraints:
      system_prompt_location: message_role
      forbid_unknown_top_level_fields: true
      mutually_exclusive:
        - ["n", "stream"]  # Cannot generate multiple completions in streaming mode
      resolution_preferences:
        - "stream"
        - "n"
      prompt_truncation:
        default_mode: "AUTO"
        supported_modes:
          - "OFF"
          - "AUTO"
          - "AUTO_PRESERVE_ORDER"
      limits:
        max_tool_schema_bytes: 100000
        max_system_prompt_bytes: 50000
    
    mappings:
      paths:
        # Direct mappings (same path in source and target)
        "$.messages": "$.messages"
        "$.model": "$.model"
        "$.temperature": "$.temperature"
        "$.top_p": "$.top_p"
        "$.max_tokens": "$.max_tokens"
        "$.frequency_penalty": "$.frequency_penalty"
        "$.presence_penalty": "$.presence_penalty"
        "$.stop": "$.stop"
        "$.seed": "$.seed"
        "$.tools": "$.tools"
        "$.tool_choice": "$.tool_choice"
        "$.response_format": "$.response_format"
        "$.stream": "$.stream"
        "$.stream_options": "$.stream_options"
        "$.user": "$.user"
        "$.n": "$.n"
        "$.logit_bias": "$.logit_bias"
        "$.logprobs": "$.logprobs"
        "$.top_logprobs": "$.top_logprobs"
        
        # Renamed/relocated mappings
        "$.limits.max_output_tokens": "$.max_tokens"
        "$.sampling.temperature": "$.temperature"
        "$.sampling.top_p": "$.top_p"
        "$.sampling.frequency_penalty": "$.frequency_penalty"
        "$.sampling.presence_penalty": "$.presence_penalty"
        "$.sampling.seed": "$.seed"
        
      flags:
        # Feature flags that modify behavior
        strict_tools:
          condition: "$.strict_mode == 'strict'"
          apply:
            path: "$.tools[*].function.strict"
            value: true
        
        include_usage:
          condition: "$.include_usage == true"
          apply:
            path: "$.stream_options.include_usage"
            value: true
    
    rag_config:
      # RAG configuration if/when OpenAI supports it
      connector_param: "retrieval_config"
      document_param: "documents"
      search_queries_param: "search_queries_only"
      web_search_connector: "web_search"
    
    conversation_management:
      # Conversation persistence configuration
      conversation_id_param: "thread_id"
      history_param: "messages"
      supports_branching: false
    
    response_normalization:
      sync:
        content_path: "choices[0].message.content"
        finish_reason_path: "choices[0].finish_reason"
        finish_reason_map:
          stop: "stop"
          length: "max_tokens"
          content_filter: "content_filter"
          tool_calls: "tool_calls"
          function_call: "tool_calls"
      
      stream:
        protocol: sse
        event_selector:
          type_path: "object"
          routes:
            - when: "chat.completion.chunk"
              emit: "delta"
              text_path: "choices[0].delta.content"
            - when: "chat.completion"
              emit: "stop"
              text_path: "choices[0].message.content"
            - when: "error"
              emit: "error"
              text_path: "error.message"

  - id: gpt-5-preview
    aliases:
      - gpt-5-turbo-preview
      - gpt-5-1106-preview
    family: gpt-5
    
    # Inherits most configuration from gpt-5
    endpoints:
      chat_completion:
        method: POST
        path: "/v1/chat/completions"
        protocol: https
      streaming_chat_completion:
        method: POST
        path: "/v1/chat/completions"
        protocol: https
        headers:
          Accept: "text/event-stream"
    
    input_modes:
      messages: true
      single_text: false
      images: true
      audio: false
      video: false
      documents: false
    
    tooling:
      tools_supported: true
      parallel_tool_calls_default: true
      can_disable_parallel_tool_calls: true
      tool_choice_modes:
        - auto
        - none
        - required
        - specific
      strict_tools_support: true
      disable_switch:
        path: "parallel_tool_calls"
        value: false
    
    json_output:
      native_param: true
      strategy: json_schema
      notes: "Preview model with experimental features"
    
    parameters:
      # Inherits all parameters from gpt-5 with same constraints
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 1
      top_p:
        type: number
        minimum: 0
        maximum: 1
        default: 1
      frequency_penalty:
        type: number
        minimum: -2
        maximum: 2
        default: 0
      presence_penalty:
        type: number
        minimum: -2
        maximum: 2
        default: 0
      max_tokens:
        type: integer
        minimum: 1
        maximum: 128000
        default: null
      n:
        type: integer
        minimum: 1
        maximum: 10
        default: 1
      stop:
        type: array
        items:
          type: string
        maxItems: 4
      seed:
        type: integer
        minimum: 0
        maximum: 2147483647
      response_format:
        type: object
      tools:
        type: array
      tool_choice: {}
      parallel_tool_calls:
        type: boolean
        default: true
      stream:
        type: boolean
        default: false
      stream_options:
        type: object
      modalities:
        type: array
        default: ["text"]
      logit_bias:
        type: object
      logprobs:
        type: boolean
        default: false
      top_logprobs:
        type: integer
      user:
        type: string
    
    constraints:
      system_prompt_location: message_role
      forbid_unknown_top_level_fields: true
      mutually_exclusive:
        - ["n", "stream"]
      resolution_preferences:
        - "stream"
        - "n"
      prompt_truncation:
        default_mode: "AUTO"
        supported_modes:
          - "OFF"
          - "AUTO"
          - "AUTO_PRESERVE_ORDER"
      limits:
        max_tool_schema_bytes: 100000
        max_system_prompt_bytes: 50000
    
    mappings:
      paths:
        "$.messages": "$.messages"
        "$.model": "$.model"
        "$.temperature": "$.temperature"
        "$.top_p": "$.top_p"
        "$.max_tokens": "$.max_tokens"
        "$.frequency_penalty": "$.frequency_penalty"
        "$.presence_penalty": "$.presence_penalty"
        "$.stop": "$.stop"
        "$.seed": "$.seed"
        "$.tools": "$.tools"
        "$.tool_choice": "$.tool_choice"
        "$.response_format": "$.response_format"
        "$.stream": "$.stream"
        "$.stream_options": "$.stream_options"
        "$.user": "$.user"
        "$.n": "$.n"
        "$.logit_bias": "$.logit_bias"
        "$.logprobs": "$.logprobs"
        "$.top_logprobs": "$.top_logprobs"
        "$.limits.max_output_tokens": "$.max_tokens"
        "$.sampling.temperature": "$.temperature"
        "$.sampling.top_p": "$.top_p"
        "$.sampling.frequency_penalty": "$.frequency_penalty"
        "$.sampling.presence_penalty": "$.presence_penalty"
        "$.sampling.seed": "$.seed"
      
      flags:
        strict_tools:
          condition: "$.strict_mode == 'strict'"
          apply:
            path: "$.tools[*].function.strict"
            value: true
        include_usage:
          condition: "$.include_usage == true"
          apply:
            path: "$.stream_options.include_usage"
            value: true
    
    rag_config:
      connector_param: "retrieval_config"
      document_param: "documents"
      search_queries_param: "search_queries_only"
      web_search_connector: "web_search"
    
    conversation_management:
      conversation_id_param: "thread_id"
      history_param: "messages"
      supports_branching: false
    
    response_normalization:
      sync:
        content_path: "choices[0].message.content"
        finish_reason_path: "choices[0].finish_reason"
        finish_reason_map:
          stop: "stop"
          length: "max_tokens"
          content_filter: "content_filter"
          tool_calls: "tool_calls"
          function_call: "tool_calls"
      
      stream:
        protocol: sse
        event_selector:
          type_path: "object"
          routes:
            - when: "chat.completion.chunk"
              emit: "delta"
              text_path: "choices[0].delta.content"
            - when: "chat.completion"
              emit: "stop"
              text_path: "choices[0].message.content"
            - when: "error"
              emit: "error"
              text_path: "error.message"

notes: |
  OpenAI GPT-5 Provider Specification
  
  This specification defines the configuration for OpenAI's GPT-5 models,
  using the standard chat completions endpoint (/v1/chat/completions).
  
  Key features:
  - Full tool/function calling support with strict mode
  - JSON Schema response format
  - Streaming with Server-Sent Events (SSE)
  - Image input support
  - Advanced sampling parameters
  
  Authentication requires an OpenAI API key set as the OPENAI_API_KEY
  environment variable.
  
  Note: GPT-5 models are defined in OpenAI's SDK but not yet publicly available.
  This specification is based on the expected model capabilities following
  the patterns established by GPT-4 models.