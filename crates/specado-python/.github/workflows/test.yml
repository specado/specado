name: Test Python Bindings

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        default: true
        override: true
        
    - name: Cache Rust dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        cd crates/specado-python
        pip install -e ".[dev,test]"
        
    - name: Lint with ruff
      run: |
        cd crates/specado-python
        ruff check python/ tests/
        
    - name: Format check with black
      run: |
        cd crates/specado-python
        black --check python/ tests/
        
    - name: Type check with mypy
      run: |
        cd crates/specado-python
        mypy python/specado/
        
    - name: Build extension
      run: |
        cd crates/specado-python
        maturin develop
        
    - name: Test with pytest
      run: |
        cd crates/specado-python
        pytest tests/ -v --tb=short
        
    - name: Test with pytest (coverage)
      if: matrix.python-version == '3.11' && matrix.os == 'ubuntu-latest'
      run: |
        cd crates/specado-python
        pip install pytest-cov
        pytest tests/ --cov=specado --cov-report=xml --cov-report=term
        
    - name: Upload coverage reports
      if: matrix.python-version == '3.11' && matrix.os == 'ubuntu-latest'
      uses: codecov/codecov-action@v3
      with:
        file: crates/specado-python/coverage.xml
        flags: python-bindings
        name: python-${{ matrix.python-version }}-${{ matrix.os }}
        
  test-memory:
    name: Memory leak tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        default: true
        override: true
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        cd crates/specado-python
        pip install -e ".[dev,test]"
        pip install psutil memory-profiler
        
    - name: Build extension
      run: |
        cd crates/specado-python
        maturin develop
        
    - name: Memory leak tests
      run: |
        cd crates/specado-python
        python -c "
        import gc
        import psutil
        import specado
        
        def memory_usage():
            return psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        print('Running memory leak tests...')
        
        # Test 1: Repeated object creation
        initial_memory = memory_usage()
        for i in range(1000):
            msg = specado.Message('user', f'Message {i}')
            prompt = specado.PromptSpec(
                model_class='Chat',
                messages=[msg],
                strict_mode='warn'
            )
            del msg, prompt
        
        gc.collect()
        after_creation = memory_usage()
        
        print(f'Initial memory: {initial_memory:.2f} MB')
        print(f'After 1000 creations: {after_creation:.2f} MB')
        print(f'Memory increase: {after_creation - initial_memory:.2f} MB')
        
        # Memory increase should be minimal (< 10 MB)
        assert after_creation - initial_memory < 10, f'Potential memory leak: {after_creation - initial_memory:.2f} MB increase'
        
        print('✅ Memory leak tests passed')
        "
        
  benchmark:
    name: Performance benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        default: true
        override: true
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        cd crates/specado-python
        pip install -e ".[dev,test]"
        pip install pytest-benchmark
        
    - name: Build extension (release)
      run: |
        cd crates/specado-python
        maturin develop --release
        
    - name: Run benchmarks
      run: |
        cd crates/specado-python
        pytest tests/ -m benchmark --benchmark-only --benchmark-sort=mean
        
    - name: Performance regression tests
      run: |
        cd crates/specado-python
        python -c "
        import time
        import specado
        
        print('Running performance tests...')
        
        # Test translation performance
        msg = specado.Message('user', 'Hello, world!')
        prompt = specado.PromptSpec(
            model_class='Chat',
            messages=[msg],
            strict_mode='warn'
        )
        
        # Warm up
        for _ in range(10):
            result = specado.validate(prompt, 'prompt')
        
        # Measure validation performance
        start_time = time.time()
        for _ in range(1000):
            result = specado.validate(prompt, 'prompt')
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 1000 * 1000  # ms
        print(f'Average validation time: {avg_time:.3f} ms')
        
        # Should be fast (< 1ms per validation)
        assert avg_time < 1.0, f'Validation too slow: {avg_time:.3f} ms'
        
        print('✅ Performance tests passed')
        "
        
  integration:
    name: Integration tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        default: true
        override: true
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        cd crates/specado-python
        pip install -e ".[dev,test]"
        
    - name: Build extension
      run: |
        cd crates/specado-python
        maturin develop
        
    - name: Run integration tests
      run: |
        cd crates/specado-python
        pytest tests/ -m integration -v
        
    - name: Test installation from wheel
      run: |
        cd crates/specado-python
        # Build wheel
        maturin build --release
        
        # Create fresh environment and test installation
        python -m venv test_env
        source test_env/bin/activate
        
        # Install from wheel
        pip install target/wheels/*.whl
        
        # Test import and basic functionality
        python -c "
        import specado
        print('✅ Wheel installation successful')
        print(f'Version: {specado.version()}')
        
        # Test basic functionality
        msg = specado.Message('user', 'Test')
        prompt = specado.PromptSpec(
            model_class='Chat',
            messages=[msg],
            strict_mode='warn'
        )
        result = specado.validate(prompt, 'prompt')
        assert result.is_valid
        print('✅ Basic functionality test passed')
        "
        
  docs:
    name: Documentation tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        default: true
        override: true
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        cd crates/specado-python
        pip install -e ".[dev]"
        
    - name: Build extension
      run: |
        cd crates/specado-python
        maturin develop
        
    - name: Test README examples
      run: |
        cd crates/specado-python
        python -c "
        # Test that README examples work
        import specado
        
        # Basic example from README
        prompt = specado.PromptSpec(
            model_class='Chat',
            messages=[
                specado.Message('system', 'You are a helpful assistant.'),
                specado.Message('user', 'What is the weather like?')
            ],
            strict_mode='warn'
        )
        
        # Validation example
        prompt_dict = {
            'model_class': 'Chat',
            'messages': [
                {'role': 'user', 'content': 'Hello, world!'}
            ],
            'strict_mode': 'warn'
        }
        
        validation_result = specado.validate(prompt_dict, 'prompt')
        assert validation_result.is_valid
        
        print('✅ README examples work correctly')
        "
        
    - name: Check docstrings
      run: |
        cd crates/specado-python
        python -c "
        import specado
        import inspect
        
        # Check that main functions have docstrings
        functions = [specado.translate, specado.validate, specado.run_sync]
        
        for func in functions:
            assert func.__doc__ is not None, f'{func.__name__} missing docstring'
            assert len(func.__doc__.strip()) > 10, f'{func.__name__} docstring too short'
        
        print('✅ Docstring checks passed')
        "